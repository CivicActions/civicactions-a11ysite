#!/usr/bin/env python3
"""Restore stub pages from the original CivicActions/accessibility repo.

Default: dry-run. Writes restored bodies (with existing front matter) to tmp/restored/<path>.
Use --apply to overwrite destination files in-place.

Assumptions:
- Mapping CSV: migration_plan/*Restructure plan.csv
- Stub list: stub_pages_report.csv (generated by verify_restructure_and_stubs.py)
- Source content: https://github.com/CivicActions/accessibility/ (main branch)
"""

import argparse
import csv
import json
import sys
from pathlib import Path
from urllib import error, request

ROOT = Path(__file__).resolve().parent.parent
TMP_DIR = ROOT / "tmp" / "restored"
STUBS_PATH = ROOT / "stub_pages_report.csv"
GITHUB_RAW_BASE = "https://raw.githubusercontent.com/CivicActions/accessibility/main/"
GITHUB_TREE_API = "https://api.github.com/repos/CivicActions/accessibility/git/trees/main?recursive=1"
REPO_TREE_CACHE = None


def find_restructure_csv() -> Path:
    plan_dir = ROOT / "migration_plan"
    for path in plan_dir.glob("*Restructure plan.csv"):
        return path
    raise FileNotFoundError("Restructure plan CSV not found under migration_plan/")


def read_csv_rows(path: Path):
    with path.open("r", encoding="utf-8-sig", newline="") as f:
        yield from csv.DictReader(f)


def load_dest_to_source() -> dict:
    csv_path = find_restructure_csv()
    mapping = {}
    for row in read_csv_rows(csv_path):
        dest = (row.get("üìç Destination URL") or "").strip()
        src = (row.get("‚ñ∂Ô∏è Source URL") or "").strip()
        if dest:
            mapping[dest] = src
    return mapping


def parse_front_matter(text: str):
    if not text.startswith("---"):
        return None, text
    parts = text.split("\n---\n", 2)
    if len(parts) < 3:
        return None, text
    fm = "---\n" + parts[1] + "\n---\n"
    body = parts[2]
    return fm, body


def strip_front_matter(text: str) -> str:
    fm, body = parse_front_matter(text)
    return body if fm else text


def load_repo_tree() -> list[str]:
    """Fetch the repo tree once so we can map slugs to real file paths."""

    global REPO_TREE_CACHE
    if REPO_TREE_CACHE is not None:
        return REPO_TREE_CACHE

    with request.urlopen(GITHUB_TREE_API) as resp:
        data = json.load(resp)
    REPO_TREE_CACHE = [item["path"] for item in data.get("tree", []) if item.get("type") == "blob"]
    return REPO_TREE_CACHE


def candidate_paths_from_source_url(source_url: str):
    if not source_url:
        return []

    tree = load_repo_tree()

    # Strip scheme/host, drop query/fragment.
    path_part = source_url.split("//", 1)[-1]
    path_part = path_part.split("/", 1)[1] if "/" in path_part else ""
    path_part = path_part.split("?", 1)[0].split("#", 1)[0]
    path_part = path_part.strip("/")

    for ext in (".html", ".htm", ".php"):
        if path_part.lower().endswith(ext):
            path_part = path_part[: -len(ext)]
            break

    slug = path_part.rstrip("/").split("/")[-1] if path_part else ""
    slug_variants = [slug]
    if "-" in slug:
        slug_variants.append(slug.split("-")[-1])
    candidates: list[str] = []

    def add(path: str):
        if path in tree and path not in candidates:
            candidates.append(path)

    # Blog posts live under _posts/YYYY-MM-DD-slug.md; match by slug tail.
    if path_part.startswith("posts/") or path_part.startswith("news/"):
        suffix = f"-{slug}.md"
        for path in tree:
            if path.startswith("_posts/") and path.endswith(suffix):
                add(path)

    # Try direct path matches.
    if path_part:
        for path in (f"{path_part}.md", f"{path_part}/index.md", path_part):
            add(path)
    else:
        add("index.md")

    if path_part.lower() in {"acr", "vpat"}:
        add("ACR/Readme.md")

    # Role URLs often collapse into _roles/<name>.md; try slug variants.
    if path_part.startswith("roles/"):
        for candidate_slug in slug_variants:
            add(f"_roles/{candidate_slug}.md")

    # Fallback: any file ending with the slug.
    if not candidates and slug:
        for candidate_slug in slug_variants:
            suffixes = (f"/{candidate_slug}.md", f"-{candidate_slug}.md", f"/{candidate_slug}/index.md")
            for path in tree:
                if path.endswith(suffixes) or path == f"{candidate_slug}.md":
                    add(path)

    return candidates


def fetch_source_body(source_url: str):
    for rel in candidate_paths_from_source_url(source_url):
        url = GITHUB_RAW_BASE + rel
        try:
            with request.urlopen(url) as resp:
                if resp.status == 200:
                    text = resp.read().decode("utf-8", errors="ignore")
                    return strip_front_matter(text), url
        except error.HTTPError:
            continue
        except error.URLError:
            continue
    return None, None


def merge_content(dest_path: Path, new_body: str):
    current = dest_path.read_text(encoding="utf-8") if dest_path.exists() else ""
    fm, _ = parse_front_matter(current)
    if fm is None:
        return new_body.rstrip() + "\n"
    return (fm + new_body).rstrip() + "\n"


def write_output(dest_path: Path, content: str, apply: bool):
    if apply:
        dest_path.parent.mkdir(parents=True, exist_ok=True)
        dest_path.write_text(content, encoding="utf-8")
    else:
        out_path = TMP_DIR / dest_path.relative_to(ROOT)
        out_path.parent.mkdir(parents=True, exist_ok=True)
        out_path.write_text(content, encoding="utf-8")


def main(argv=None):
    parser = argparse.ArgumentParser(description="Restore stub pages from source repo")
    parser.add_argument("--apply", action="store_true", help="Overwrite destination files instead of dry-run")
    parser.add_argument("--limit", type=int, default=None, help="Limit number of pages to process")
    args = parser.parse_args(argv)

    dest_to_source = load_dest_to_source()
    if not STUBS_PATH.exists():
        raise SystemExit("stub_pages_report.csv not found; run verify_restructure_and_stubs.py first")

    stubs = list(read_csv_rows(STUBS_PATH))
    processed = 0
    restored = 0
    skipped = 0
    for row in stubs:
        if args.limit and processed >= args.limit:
            break
        processed += 1
        dest_url = (row.get("destination_url") or "").strip()
        rel_path = (row.get("path") or "").strip()
        if not dest_url or not rel_path:
            skipped += 1
            continue
        source_url = dest_to_source.get(dest_url, "")
        body, used_url = fetch_source_body(source_url)
        if body is None:
            skipped += 1
            print(f"[skip] no source found for {dest_url} (source: {source_url})")
            continue
        dest_path = ROOT / rel_path
        merged = merge_content(dest_path, "\n" + body.lstrip())
        write_output(dest_path, merged, apply=args.apply)
        restored += 1
        print(f"[ok] {'applied' if args.apply else 'written'} {rel_path} from {used_url}")

    print(f"Processed: {processed}, restored: {restored}, skipped: {skipped}, apply={args.apply}")


if __name__ == "__main__":
    main()
